---
title: "R_tutorial3-uncommented"
author: "Sunwoo Jeong"
date: "6/2/2020"
output:
  pdf_document: default
  html_document: default
---

# Binary dependent variables

We'll again use the factivity and prosody data as a case study. While the main dependent variable is not binary, it can be construed as a categorical variable (a factor variable with multiple discrete levels)

```{r setup, include=FALSE}
## Call this in whenever you want to use associated packages such as ggplot2, tidyr, etc.
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)

## The data: target data is a .csv containing the cleaned target data from the previous tutorial
dat <- read.csv('veridicality-target-cleaned.csv')
head(dat)

## First, make sure that R treats relevant dependent variables as factors
summary(dat$veridicality)
dat$veridicality <- as.factor(dat$veridicality)
summary(dat$veridicality)

```

# Basic summaries and visualizations

Again we use the `mutate` and `group_by` command. The relevant built-in functions are `n()` and `sum()`

```{r}
## Creating count/percent summary

sum_dat <- dat %>%
  group_by(verbtype, complementizer, prosody, veridicality) %>% 
  summarise(count=n()) %>% 
  mutate(perc=count/sum(count), n=sum(count))

## Some possible visualizations

p <- sum_dat %>%
  ggplot(aes(x = verbtype, y = perc, fill = veridicality)) +
  geom_bar(stat = "identity") +
  facet_grid(prosody ~ complementizer)

```




# Basi regression models

It would take us too far afield to cover all the theoretical background for linear models and mixed effects models. Please consult the readings I've posted for a gentle introduction, as well as a variety of resources available on the web. The course being offered next semester will also provide an in-depth training. Today, we will focus on the practical aspects of fitting linear regression models, logistic regression models, and mixed effects models. Let us first call in the data -- along with the R packages needed for fitting regression models.

```{r setup, include=FALSE}
## Install these packages if they aren't already installed
# install.packages("lme4")
# install.packages("lmerTest")

## Call them in
library(lme4)
library(lmerTest)

## The data
datc <- read.csv('asmt1-korean-stops.csv')
datv <- read.csv('asmt1-korean-vowels.csv')
```


# Linear regression: the vowel data

Say that we want to examine if height and speaker gender is a significant predictor or F0.
A simple linear model which tests this would be as follows.
(syntax: dependent variable ~ predictor(factor)1 + predictor(factor)2, ..., data=data)


```{r}
m1 <- lm(F0 ~ ht + sp_gender, data=datv)
summary(m1) 
```

Any other things you might want to include in the model? Try fitting models with F1, F2, and F3 as dependent variables as well.

```{r}
m2 <- lm(F0 ~ ht + sp_gender + wpos + vpos, data=datv)
summary(m2) 

m3 <- lm(F1 ~ ht + sp_gender + vtype, data=datv)
summary(m3)
```
A sample report: Speaker gender was a significant predictor of F0: male speakers elicited significantly lower F0 than female speakers (beta = -148, SE = 17, t = -8, p < .001).


# Logistic or ordinal regression

Logistic regression is the way to go when the dependent variable involves binary choices. Ordinal regression is the way to go when the dependent variable involves factor with (ordered) categorical levels (e.g., a likert scale task). Mixed effects models are often used (positing random slopes and intercepts) to explain away any pontential (unaccounted) random effects. Usually, items and participants are included as random effects.

```{r}
dat_jul <-
  dat %>%
  filter(complementizer == "jul1",
         complementizer == "jul2")


dat_jul$veridicality <- as.factor(dat_jul$veridicality)

m1 <- clmm(veridicality ~ prosody * verbtype + (1|item) + (1|uniqueIdentifier), data=dat_jul)
summary(m1)
```

